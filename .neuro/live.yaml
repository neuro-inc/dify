kind: live
title: dify

# other files from https://github.com/langgenius/dify

defaults:
  life_span: 100d

images:
  dify_api:
    ref: langgenius/dify-api:0.6.7
  dify_web:
    ref: langgenius/dify-web:0.6.7
  dify_sandbox:
    ref: langgenius/dify-sandbox:0.1.0

  api:
    ref: image:${{ flow.project_id }}/api
    context: ${{ flow.workspace }}/api
    dockerfile: ${{ flow.workspace }}/api/Dockerfile
    build_preset: cpu-8-20
  web:
    ref: image:${{ flow.project_id }}/web
    context: ${{ flow.workspace }}/web
    dockerfile: ${{ flow.workspace }}/web/Dockerfile
    build_preset: cpu-8-20

volumes:
  data:
    remote: storage:$[[ flow.project_id ]]/storage
    mount: /app/api/storage
    local: local_data
  pgdata:
    # remote: disk:dify-pgdata  # onprem
    remote: storage:$[[ flow.project_id ]]/pgdata # scottdc
    mount: /var/lib/postgresql/data
    local: pgdata
  redis:
    # remote: disk:dify-redis  # onprem
    remote: storage:$[[ flow.project_id ]]/redis # scottdc
    mount: /data
  nginx_cfg:
    remote: storage:$[[ flow.project_id ]]/nginx_cfg
    mount: /tmp/nginx_cfg/
    local: docker/nginx/
  omaha:
    remote: storage:$[[ flow.project_id ]]/omaha/src
    mount: /etc/nginx/html
    local: omaha/src/
  nginx_cfg_omaha:
    remote: storage:$[[ flow.project_id ]]/omaha/nginx_cfg
    mount: /tmp/nginx_cfg/
    local: omaha/nginx/
  cache:
    remote: storage:private_gpt/cache
    mount: /root/.cache/huggingface
  tei_cache:
    remote: storage:tei/cache
    mount: /data
  ollama_models:
    remote: storage:$[[ flow.project_id ]]/ollama_models
    mount: /root/.ollama
    local: models


jobs:
  api:
    image: ${{ images.api.ref }}
    # image: ${{ images.dify_api.ref }}
    name: api
    preset: cpu-large
    http_port: "8080"
    detach: true
    browse: false
    volumes:
      - ${{ volumes.data.ref_rw }}
    env:
      MODE: api
      LOG_LEVEL: INFO
      SECRET_KEY: NOT_SECRET
      INIT_PASSWORD: apolo
      CONSOLE_WEB_URL: ""
      CONSOLE_API_URL: ""
      SERVICE_API_URL: ""
      APP_WEB_URL: ""
      FILES_URL: ""
      MIGRATION_ENABLED: "true"
      # DB
      DB_USERNAME: postgres
      DB_PASSWORD: difyai123456
      DB_HOST: ${{ inspect_job('pgvector').internal_hostname_named }}
      DB_PORT: 5432
      DB_DATABASE: dify
      # Redis
      REDIS_HOST: ${{ inspect_job('redis').internal_hostname_named }}
      REDIS_PORT: 6379
      REDIS_USERNAME: ""
      REDIS_PASSWORD: difyai123456
      REDIS_USE_SSL: "false"
      REDIS_DB: 0
      CELERY_BROKER_URL: redis://:difyai123456@${{ inspect_job('redis').internal_hostname_named }}:6379/1
      # CORS
      WEB_API_CORS_ALLOW_ORIGINS: "*"
      CONSOLE_CORS_ALLOW_ORIGINS: "*"
      STORAGE_TYPE: local
      STORAGE_LOCAL_PATH: ${{ volumes.data.mount }}
      # Vector store
      VECTOR_STORE: pgvector
      PGVECTOR_HOST: ${{ inspect_job('pgvector').internal_hostname_named }}
      PGVECTOR_PORT: 5432
      PGVECTOR_USER: postgres
      PGVECTOR_PASSWORD: difyai123456
      PGVECTOR_DATABASE: dify
      # The sandbox service endpoint.
      # CODE_EXECUTION_ENDPOINT: http://${{ inspect_job('sandbox').internal_hostname_named }}:8194
      CODE_EXECUTION_API_KEY: dify-sandbox
      CODE_MAX_NUMBER: 9223372036854775807
      CODE_MIN_NUMBER: -9223372036854775808
      CODE_MAX_STRING_LENGTH: 80000
      TEMPLATE_TRANSFORM_MAX_LENGTH: 80000
      CODE_MAX_STRING_ARRAY_LENGTH: 30
      CODE_MAX_OBJECT_ARRAY_LENGTH: 30
      CODE_MAX_NUMBER_ARRAY_LENGTH: 1000
      # misc
      SENTRY_DSN: ""

  worker:
    image: ${{ images.api.ref }}
    # image: ${{ images.dify_api.ref }}
    preset: H100x1
    # preset: cpu-large
    detach: true
    volumes:
      - ${{ volumes.data.ref_rw }}
    env:
      CONSOLE_WEB_URL: ""
      MODE: worker
      ## All the same as for API --->
      LOG_LEVEL: INFO
      SECRET_KEY: NOT_SECRET
      INIT_PASSWORD: apolo
      SERVICE_API_URL: ""
      APP_WEB_URL: ""
      FILES_URL: ""
      MIGRATION_ENABLED: "true"
      # DB
      DB_USERNAME: postgres
      DB_PASSWORD: difyai123456
      DB_HOST: ${{ inspect_job('pgvector').internal_hostname_named }}
      DB_PORT: 5432
      DB_DATABASE: dify
      # Redis
      REDIS_HOST: ${{ inspect_job('redis').internal_hostname_named }}
      REDIS_PORT: 6379
      REDIS_USERNAME: ""
      REDIS_PASSWORD: difyai123456
      REDIS_USE_SSL: "false"
      REDIS_DB: 0
      CELERY_BROKER_URL: redis://:difyai123456@${{ inspect_job('redis').internal_hostname_named }}:6379/1
      # CORS
      WEB_API_CORS_ALLOW_ORIGINS: "*"
      CONSOLE_CORS_ALLOW_ORIGINS: "*"
      STORAGE_TYPE: local
      STORAGE_LOCAL_PATH: ${{ volumes.data.mount }}
      # Vector store
      VECTOR_STORE: pgvector
      PGVECTOR_HOST: ${{ inspect_job('pgvector').internal_hostname_named }}
      PGVECTOR_PORT: 5432
      PGVECTOR_USER: postgres
      PGVECTOR_PASSWORD: difyai123456
      PGVECTOR_DATABASE: dify
      # The sandbox service endpoint.
      # CODE_EXECUTION_ENDPOINT: http://${{ inspect_job('sandbox').internal_hostname_named }}:8194
      CODE_EXECUTION_API_KEY: dify-sandbox
      CODE_MAX_NUMBER: 9223372036854775807
      CODE_MIN_NUMBER: -9223372036854775808
      CODE_MAX_STRING_LENGTH: 80000
      TEMPLATE_TRANSFORM_MAX_LENGTH: 80000
      CODE_MAX_STRING_ARRAY_LENGTH: 30
      CODE_MAX_OBJECT_ARRAY_LENGTH: 30
      CODE_MAX_NUMBER_ARRAY_LENGTH: 1000
      # misc
      SENTRY_DSN: ""

  web:
    image: ${{ images.web.ref }}
    # image: ${{ images.dify_web.ref }}
    name: web
    http_port: "3000"
    detach: true
    env:
      CONSOLE_API_URL: ""
      APP_API_URL: ""
      SENTRY_DSN: ""

  pgvector:
    image: pgvector/pgvector:pg16
    detach: true
    preset: cpu-large
    env:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: difyai123456
      POSTGRES_DB: dify
      PGDATA: ${{ volumes.pgdata.mount }}
    volumes:
      - ${{ volumes.pgdata.ref_rw }}

  redis:
    image: redis:6-alpine
    preset: cpu-large
    detach: true
    volumes:
      - ${{ volumes.redis.ref_rw }}
    cmd: redis-server --requirepass difyai123456

  nginx:
    image: nginx:latest
    detach: true
    http_port: 80
    http_auth: true
    life_span: 300d
    env:
      DIFY_API_HOST: ${{ inspect_job('api').internal_hostname_named }}
      DIFY_WEB_HOST: ${{ inspect_job('web').internal_hostname_named }}
    volumes:
      - ${{ upload(volumes.nginx_cfg).ref_rw }}
    bash: |
      cp -r ${{ volumes.nginx_cfg.mount }}/* /etc/nginx/
      envsubst '$DIFY_API_HOST $DIFY_WEB_HOST' < ${{ volumes.nginx_cfg.mount }}/conf.d/default.conf > /etc/nginx/conf.d/default.conf
      nginx -g 'daemon off;'
  nginx_omaha:
    image: nginx:latest
    detach: false
    http_port: 80
    http_auth: False
    life_span: 300d
    env:
      DIFY_API_HOST: ${{ inspect_job('api').internal_hostname_named }}
      DIFY_WEB_HOST: ${{ inspect_job('web').internal_hostname_named }}
      DIFY_NGINX_HOST: ${{ inspect_job('nginx').internal_hostname_named }}
    volumes:
      - ${{ upload(volumes.nginx_cfg_omaha).ref_rw }}
      - ${{ upload(volumes.omaha).ref_ro }}
      - storage:$[[ flow.project_id ]]/omaha/nginx_cfg/.htpasswd:/etc/nginx/.htpasswd
    bash: |
      cp -r ${{ volumes.nginx_cfg_omaha.mount }}/* /etc/nginx/
      envsubst '$DIFY_API_HOST $DIFY_WEB_HOST $DIFY_NGINX_HOST' < ${{ volumes.nginx_cfg_omaha.mount }}/conf.d/default.conf > /etc/nginx/conf.d/default.conf
      nginx -g 'daemon off;'
  vllm:
    image: vllm/vllm-openai:v0.4.2
    env:
      HF_TOKEN: secret:HF_TOKEN
    volumes:
      - ${{volumes.cache.ref}}
    params:
      preset: H100x4
    preset: ${{params.preset}}
    http_port: 8000
    life_span: 30d
    cmd: --model meta-llama/Meta-Llama-3-70B-Instruct --tokenizer meta-llama/Meta-Llama-3-70B-Instruct --dtype=half --tensor-parallel-size=4
    
  ollama_embeddings:
    image: ollama/ollama:0.2.5
    volumes:
      - ${{ volumes.ollama_models.ref_rw }}
    preset: H100x1
    detach: true
    env:
      MODEL: "nomic-embed-text"
      OLLAMA_DEBUG: 1
      OLLAMA_KEEP_ALIVE: -1
    http_port: "11434"
    entrypoint: "bash -c 'ollama serve & sleep 10 && ollama pull ${MODEL} && sleep infinity'"

  hf_tei:
    image: ghcr.io/huggingface/text-embeddings-inference:hopper-1.5
    preset: H100x1
    params:
      model: Salesforce/SFR-Embedding-2_R
    env:
      RUST_BACKTRACE: full
    http_port: 80
    volumes:
      - ${{volumes.cache.ref}}
      - secret:HF_TOKEN:/root/.cache/huggingface/token
    cmd:  --model-id $[[params.model]]


